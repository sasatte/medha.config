{
  "version": "1.0",
  "description": "Medha AI Model Catalog",
  "models": [
    {
      "id": "silero-vad-v4",
      "name": "Silero VAD v4",
      "version": "4.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/silero-vad-v4/silero.onnx",
      "sha256": "",
      "size_bytes": 1800000,
      "description": "High-quality Voice Activity Detection model from Silero",
      "tags": ["vad", "speech-detection", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "input_format": "float32",
        "window_size_ms": "30"
      },
      "requirements": {
        "min_memory_mb": 50,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "smart-turn-v3",
      "name": "SmartTurn v3.0",
      "version": "3.0",
      "type": "smart_turn",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/turn-detection/smart-turn-v3/smart-turn-v3.onnx",
      "sha256": "",
      "size_bytes": 3500000,
      "description": "Turn-taking detection model for conversational AI",
      "tags": ["turn-taking", "conversation", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "window_duration_sec": "8.2",
        "frame_shift_sec": "0.05"
      },
      "requirements": {
        "min_memory_mb": 100,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "titanet-large",
      "name": "CAM++ Speaker Verification",
      "version": "1.0",
      "type": "speaker_verification",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/speaker-verification/camplus/camplus.onnx",
      "sha256": "",
      "size_bytes": 23000000,
      "description": "CAM++ model for speaker verification and identification",
      "tags": ["speaker-verification", "speaker-embedding", "camplus"],
      "metadata": {
        "sample_rate": "16000",
        "embedding_size": "192",
        "input_duration_sec": "variable"
      },
      "requirements": {
        "min_memory_mb": 200,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "whisper-tiny-en",
      "name": "Whisper Tiny English (Sherpa-ONNX)",
      "version": "1.0",
      "type": "asr",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-tiny-en/encoder.onnx",
      "sha256": "",
      "size_bytes": 40000000,
      "description": "Sherpa-ONNX optimized Whisper Tiny English (INT8 quantized)",
      "tags": ["asr", "whisper", "english", "speech-to-text", "sherpa-onnx", "int8"],
      "metadata": {
        "sample_rate": "16000",
        "language": "en",
        "model_size": "tiny",
        "quantization": "int8",
        "encoder_url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-tiny-en/encoder.onnx",
        "decoder_url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-tiny-en/decoder.onnx",
        "tokens_url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-tiny-en/tokens.txt"
      },
      "requirements": {
        "min_memory_mb": 200,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "whisper-base-multilingual",
      "name": "Whisper Base Multilingual",
      "version": "1.0",
      "type": "asr",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-base-multilingual/encoder.onnx",
      "sha256": "",
      "size_bytes": 145000000,
      "description": "OpenAI Whisper Base model for multilingual ASR",
      "tags": ["asr", "whisper", "multilingual", "speech-to-text"],
      "metadata": {
        "sample_rate": "16000",
        "languages": "99",
        "model_size": "base"
      },
      "requirements": {
        "min_memory_mb": 800,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "marblenet-vad",
      "name": "MarbleNet VAD",
      "version": "1.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/marblenet/marblenet.onnx",
      "sha256": "",
      "size_bytes": 900000,
      "description": "NVIDIA MarbleNet Voice Activity Detection model",
      "tags": ["vad", "nvidia", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "frame_length_ms": "80",
        "frame_shift_ms": "10"
      },
      "requirements": {
        "min_memory_mb": 50,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "funasr-vad",
      "name": "FunASR VAD",
      "version": "1.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/funasr/funasr.onnx",
      "sha256": "",
      "size_bytes": 350000,
      "description": "Alibaba FunASR VAD model optimized for Chinese and English",
      "tags": ["vad", "chinese", "english", "alibaba"],
      "metadata": {
        "sample_rate": "16000",
        "languages": "zh,en",
        "chunk_size_ms": "200"
      },
      "requirements": {
        "min_memory_mb": 30,
        "requires_gpu": false,
        "dependencies": []
      }
    }
  ]
}

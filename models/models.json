{
  "version": "1.0",
  "description": "Medha AI Model Catalog",
  "models": [
    {
      "id": "silero-vad-v4",
      "name": "Silero VAD v4",
      "version": "4.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/silero-vad-v4/silero.onnx",
      "sha256": "",
      "size_bytes": 1800000,
      "description": "High-quality Voice Activity Detection model from Silero",
      "tags": ["vad", "speech-detection", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "input_format": "float32",
        "window_size_ms": "30"
      },
      "requirements": {
        "min_memory_mb": 50,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "smart-turn-v3",
      "name": "SmartTurn v3.0",
      "version": "3.0",
      "type": "smart_turn",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/turn-detection/smart-turn-v3/smart-turn-v3.onnx",
      "sha256": "",
      "size_bytes": 3500000,
      "description": "Turn-taking detection model for conversational AI",
      "tags": ["turn-taking", "conversation", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "window_duration_sec": "8.2",
        "frame_shift_sec": "0.05"
      },
      "requirements": {
        "min_memory_mb": 100,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "titanet-large",
      "name": "CAM++ Speaker Verification",
      "version": "1.0",
      "type": "speaker_verification",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/speaker-verification/camplus/camplus.onnx",
      "sha256": "",
      "size_bytes": 23000000,
      "description": "CAM++ model for speaker verification and identification",
      "tags": ["speaker-verification", "speaker-embedding", "camplus"],
      "metadata": {
        "sample_rate": "16000",
        "embedding_size": "192",
        "input_duration_sec": "variable"
      },
      "requirements": {
        "min_memory_mb": 200,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "whisper-tiny-en",
      "name": "Whisper Tiny English (Sherpa-ONNX)",
      "version": "1.0",
      "type": "asr",
      "format": "archive",
      "archive_type": "tar.gz",
      "extract_to_subdir": true,
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-tiny-en.tar.gz",
      "sha256": "",
      "size_bytes": 40000000,
      "description": "Sherpa-ONNX optimized Whisper Tiny English (INT8 quantized)",
      "tags": ["asr", "whisper", "english", "speech-to-text", "sherpa-onnx", "int8"],
      "metadata": {
        "sample_rate": "16000",
        "language": "en",
        "model_size": "tiny",
        "quantization": "int8",
        "files": ["encoder.onnx", "decoder.onnx", "tokens.txt"]
      },
      "requirements": {
        "min_memory_mb": 200,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "whisper-turbo",
      "name": "Whisper Turbo English (Sherpa-ONNX)",
      "version": "1.0",
      "type": "asr",
      "format": "archive",
      "archive_type": "tar.gz",
	"extract_to_subdir": true,
	"url" : "https://media.githubusercontent.com/media/sasatte/medha.config/refs/heads/main/models/asr/whisper-turbo.tar.gz",
      "sha256": "",
      "size_bytes": 40000000,
      "description": "Sherpa-ONNX optimized Whisper Turbo English (INT8 quantized)",
      "tags": ["asr", "whisper", "english", "speech-to-text", "sherpa-onnx", "int8"],
      "metadata": {
        "sample_rate": "16000",
        "language": "en",
        "model_size": "turbo",
        "quantization": "int8",
        "files": ["encoder.onnx", "decoder.onnx", "tokens.txt"]
      },
      "requirements": {
        "min_memory_mb": 200,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "whisper-base-multilingual",
      "name": "Whisper Base Multilingual",
      "version": "1.0",
      "type": "asr",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/asr/whisper-base-multilingual/encoder.onnx",
      "sha256": "",
      "size_bytes": 145000000,
      "description": "OpenAI Whisper Base model for multilingual ASR",
      "tags": ["asr", "whisper", "multilingual", "speech-to-text"],
      "metadata": {
        "sample_rate": "16000",
        "languages": "99",
        "model_size": "base"
      },
      "requirements": {
        "min_memory_mb": 800,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "marblenet-vad",
      "name": "MarbleNet VAD",
      "version": "1.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/marblenet/marblenet.onnx",
      "sha256": "",
      "size_bytes": 900000,
      "description": "NVIDIA MarbleNet Voice Activity Detection model",
      "tags": ["vad", "nvidia", "real-time"],
      "metadata": {
        "sample_rate": "16000",
        "frame_length_ms": "80",
        "frame_shift_ms": "10"
      },
      "requirements": {
        "min_memory_mb": 50,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "funasr-vad",
      "name": "FunASR VAD",
      "version": "1.0",
      "type": "vad",
      "format": "onnx",
      "url": "https://raw.githubusercontent.com/sasatte/medha.config/refs/heads/main/models/vad/funasr/funasr.onnx",
      "sha256": "",
      "size_bytes": 350000,
      "description": "Alibaba FunASR VAD model optimized for Chinese and English",
      "tags": ["vad", "chinese", "english", "alibaba"],
      "metadata": {
        "sample_rate": "16000",
        "languages": "zh,en",
        "chunk_size_ms": "200"
      },
      "requirements": {
        "min_memory_mb": 30,
        "requires_gpu": false,
        "dependencies": []
      }
    },
    {
      "id": "kokoro-multi-lang-v1_1",
      "name": "Kokoro TTS Multi-Language v1.1",
      "version": "1.1",
      "type": "tts",
      "format": "archive",
      "archive_type": "tar.bz2",
      "extract_to_subdir": true,
      "url": "https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/kokoro-multi-lang-v1_1.tar.bz2",
      "sha256": "",
      "size_bytes": 250000000,
      "description": "Kokoro multi-language TTS model supporting English and Chinese with 103 speakers",
      "tags": ["tts", "kokoro", "multilingual", "english", "chinese", "sherpa-onnx"],
      "metadata": {
        "sample_rate": "24000",
        "languages": "en,zh",
        "num_speakers": "103",
        "speaker_range": "0-102",
        "model_type": "kokoro",
        "files": ["model.onnx", "voices.bin", "tokens.txt", "espeak-ng-data", "dict", "lexicon-us-en.txt", "lexicon-zh.txt"]
      },
      "requirements": {
        "min_memory_mb": 500,
        "requires_gpu": false,
        "dependencies": ["sherpa-onnx"]
      }
    },
    {
      "id": "kokoro-multi-lang-v1_0",
      "name": "Kokoro TTS Multi-Language v1.0",
      "version": "1.0",
      "type": "tts",
      "format": "archive",
      "archive_type": "tar.bz2",
      "extract_to_subdir": true,
      "url": "https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/kokoro-multi-lang-v1_0.tar.bz2",
      "sha256": "",
      "size_bytes": 200000000,
      "description": "Kokoro multi-language TTS model supporting English and Chinese with 53 speakers",
      "tags": ["tts", "kokoro", "multilingual", "english", "chinese", "sherpa-onnx"],
      "metadata": {
        "sample_rate": "24000",
        "languages": "en,zh",
        "num_speakers": "53",
        "speaker_range": "0-52",
        "model_type": "kokoro",
        "files": ["model.onnx", "voices.bin", "tokens.txt", "espeak-ng-data", "dict", "lexicon-us-en.txt", "lexicon-zh.txt"]
      },
      "requirements": {
        "min_memory_mb": 400,
        "requires_gpu": false,
        "dependencies": ["sherpa-onnx"]
      }
    },
    {
      "id": "kokoro-en-v1_0",
      "name": "Kokoro TTS English v1.0",
      "version": "1.0",
      "type": "tts",
      "format": "archive",
      "archive_type": "tar.bz2",
      "extract_to_subdir": true,
      "url": "https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/kokoro-en-v1_0.tar.bz2",
      "sha256": "",
      "size_bytes": 150000000,
      "description": "Kokoro English-only TTS model with 11 speakers",
      "tags": ["tts", "kokoro", "english", "sherpa-onnx"],
      "metadata": {
        "sample_rate": "24000",
        "languages": "en",
        "num_speakers": "11",
        "speaker_range": "0-10",
        "model_type": "kokoro",
        "files": ["model.onnx", "voices.bin", "tokens.txt", "espeak-ng-data", "dict", "lexicon-us-en.txt"]
      },
      "requirements": {
        "min_memory_mb": 300,
        "requires_gpu": false,
        "dependencies": ["sherpa-onnx"]
      }
    }
  ]
}

